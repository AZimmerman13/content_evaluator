{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from time import time\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from six.moves import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusion(nn.Module):\n",
    "\n",
    "    def __init__(self,in_dimensions,out_dimension):\n",
    "        super(TensorFusion, self).__init__()\n",
    "        self.tensor_size=reduce(lambda x, y: x*y, in_dimensions)\n",
    "        self.linear_layer=nn.Linear(self.tensor_size,out_dimension)\n",
    "        self.in_dimensions=in_dimensions\n",
    "        self.out_dimension=out_dimension\n",
    "\n",
    "    def __call__(self,in_modalities):\n",
    "        return self.fusion(in_modalities)\n",
    "\n",
    "    def fusion(self,in_modalities):\n",
    "\n",
    "        bs=in_modalities[0].shape[0]\n",
    "        tensor_product=in_modalities[0]\n",
    "        print(tensor_product.shape)\n",
    "        #calculating the tensor product\n",
    "\n",
    "        for in_modality in in_modalities[1:]:\n",
    "            tensor_product=torch.bmm(tensor_product.unsqueeze(2),in_modality.unsqueeze(1))\n",
    "            print(tensor_product.shape)\n",
    "            tensor_product=tensor_product.view(bs,-1)\n",
    "            print(tensor_product.shape)\n",
    "            \n",
    "        return self.linear_layer(tensor_product)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Not yet implemented for nn.Sequential\")\n",
    "        exit(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 40])\n",
      "torch.Size([32, 40, 12])\n",
      "torch.Size([32, 480])\n",
      "torch.Size([32, 480, 20])\n",
      "torch.Size([32, 9600])\n",
      "Output\n",
      "tensor([ 8.5661e-04,  9.0665e-03,  6.4471e-03,  8.4692e-03,  6.0398e-04,\n",
      "        -5.7766e-03, -3.4249e-03,  3.7142e-03, -8.6104e-03, -7.7355e-03,\n",
      "         4.2412e-03, -6.5096e-03,  3.1209e-03,  2.2976e-03, -3.6813e-03,\n",
      "        -5.1081e-03,  4.5165e-03,  8.0942e-03,  4.0931e-03, -2.1696e-03,\n",
      "        -7.0976e-03, -6.3472e-03,  9.3062e-03,  8.1895e-03,  7.8201e-03,\n",
      "         9.8138e-03,  7.8073e-03, -9.8971e-03,  5.6895e-03,  7.6192e-03,\n",
      "        -7.2736e-03, -8.4121e-03,  2.2415e-03, -3.9750e-03,  8.4418e-05,\n",
      "         9.6850e-03,  2.5787e-04,  3.4080e-03, -9.6826e-03, -6.2715e-03,\n",
      "        -7.0232e-03,  5.6313e-03,  1.9578e-03,  5.7782e-03, -6.9318e-03,\n",
      "        -2.6567e-03,  5.9779e-03,  4.8833e-03, -5.5868e-03, -8.9137e-03,\n",
      "        -1.3677e-04, -8.5372e-03,  3.6871e-03,  8.8608e-03,  7.6942e-03,\n",
      "         1.7768e-03, -2.3467e-03,  7.0112e-03, -5.4393e-03, -1.7017e-03,\n",
      "         6.1227e-03,  1.8997e-03, -7.5505e-03, -1.8078e-03,  8.9608e-03,\n",
      "         8.3730e-03, -2.7874e-03,  4.9894e-03, -4.0000e-03, -8.0863e-04,\n",
      "         5.2340e-03, -9.7294e-03,  7.5195e-04, -8.5290e-03, -9.6863e-03,\n",
      "         2.2440e-03, -1.0242e-04, -9.4838e-03, -8.5653e-03, -5.2770e-03,\n",
      "        -8.3386e-03, -3.2703e-03, -6.2222e-03, -3.7471e-03,  1.0038e-02,\n",
      "        -9.2612e-03, -5.9908e-03, -7.3053e-03, -4.5935e-03, -3.2056e-03,\n",
      "        -1.9697e-03,  8.8868e-03,  7.5113e-03,  7.7356e-03,  3.0887e-03,\n",
      "         9.9931e-03, -4.1292e-03, -6.1870e-03,  2.1338e-03,  4.7779e-03],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Toy sample finished ...\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import numpy\n",
    "\n",
    "inputx=Variable(torch.Tensor(numpy.zeros([32,40])),requires_grad=True)\n",
    "inputy=Variable(torch.Tensor(numpy.array(numpy.zeros([32,12]))),requires_grad=True)\n",
    "inputz=Variable(torch.Tensor(numpy.array(numpy.zeros([32,20]))),requires_grad=True)\n",
    "modalities=[inputx,inputy,inputz]\n",
    "\n",
    "fmodel=TensorFusion([40,12,20],100)\n",
    "\n",
    "out=fmodel(modalities)\n",
    "\n",
    "print(\"Output\")\n",
    "print(out[0])\n",
    "print(\"Toy sample finished ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform, xavier_normal, orthogonal\n",
    "\n",
    "\n",
    "class SubNet(nn.Module):\n",
    "    '''\n",
    "    The subnetwork that is used in TFN for video and audio in the pre-fusion stage\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, dropout):\n",
    "        '''\n",
    "        Args:\n",
    "            in_size: input dimension\n",
    "            hidden_size: hidden layer dimension\n",
    "            dropout: dropout probability\n",
    "        Output:\n",
    "            (return value in forward) a tensor of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        super(SubNet, self).__init__()\n",
    "        self.norm = nn.BatchNorm1d(in_size)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.linear_1 = nn.Linear(in_size, hidden_size)\n",
    "        self.linear_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_3 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: tensor of shape (batch_size, in_size)\n",
    "        '''\n",
    "        normed = self.norm(x)\n",
    "        dropped = self.drop(normed)\n",
    "        y_1 = F.relu(self.linear_1(dropped))\n",
    "        y_2 = F.relu(self.linear_2(y_1))\n",
    "        y_3 = F.relu(self.linear_3(y_2))\n",
    "\n",
    "        return y_3\n",
    "\n",
    "\n",
    "class TextSubNet(nn.Module):\n",
    "    '''\n",
    "    The LSTM-based subnetwork that is used in TFN for text\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_layers=1, dropout=0.2, bidirectional=False):\n",
    "        '''\n",
    "        Args:\n",
    "            in_size: input dimension\n",
    "            hidden_size: hidden layer dimension\n",
    "            num_layers: specify the number of layers of LSTMs.\n",
    "            dropout: dropout probability\n",
    "            bidirectional: specify usage of bidirectional LSTM\n",
    "        Output:\n",
    "            (return value in forward) a tensor of shape (batch_size, out_size)\n",
    "        '''\n",
    "        super(TextSubNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(in_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: tensor of shape (batch_size, sequence_len, in_size)\n",
    "        '''\n",
    "        _, final_states = self.rnn(x)\n",
    "        h = self.dropout(final_states[0].squeeze())\n",
    "        y_1 = self.linear_1(h)\n",
    "        return y_1\n",
    "\n",
    "\n",
    "class TFN(nn.Module):\n",
    "    '''\n",
    "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
    "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, text_out, dropouts, post_fusion_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dims - a length-3 tuple, contains (audio_dim, video_dim, text_dim)\n",
    "            hidden_dims - another length-3 tuple, similar to input_dims\n",
    "            text_out - int, specifying the resulting dimensions of the text subnetwork\n",
    "            dropouts - a length-4 tuple, contains (audio_dropout, video_dropout, text_dropout, post_fusion_dropout)\n",
    "            post_fusion_dim - int, specifying the size of the sub-networks after tensorfusion\n",
    "        Output:\n",
    "            (return value in forward) a scalar value between -3 and 3\n",
    "        '''\n",
    "        super(TFN, self).__init__()\n",
    "\n",
    "        # dimensions are specified in the order of audio, video and text\n",
    "        self.img_in = input_dims[0]\n",
    "        self.text_in = input_dims[1]\n",
    "        #self.text_in = input_dims[2]\n",
    "\n",
    "        self.img_hidden = hidden_dims[0]\n",
    "#         self.video_hidden = hidden_dims[1]\n",
    "        self.text_hidden = hidden_dims[1]\n",
    "        self.text_out= text_out\n",
    "        self.post_fusion_dim = post_fusion_dim\n",
    "\n",
    "        self.img_prob = dropouts[0]\n",
    "#         self.video_prob = dropouts[1]\n",
    "        self.text_prob = dropouts[1]\n",
    "        self.post_fusion_prob = dropouts[3]\n",
    "\n",
    "        # define the pre-fusion subnetworks\n",
    "        self.img_subnet = SubNet(self.img_in, self.img_hidden, self.img_prob)\n",
    "#         self.video_subnet = SubNet(self.video_in, self.video_hidden, self.video_prob)\n",
    "        self.text_subnet = TextSubNet(self.text_in, self.text_hidden, self.text_out, dropout=self.text_prob)\n",
    "\n",
    "        # define the post_fusion layers\n",
    "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
    "        self.post_fusion_layer_1 = nn.Linear((self.text_out + 1) * (self.img_hidden + 1), self.post_fusion_dim)\n",
    "        self.post_fusion_layer_2 = nn.Linear(self.post_fusion_dim, self.post_fusion_dim)\n",
    "        self.post_fusion_layer_3 = nn.Linear(self.post_fusion_dim, 1)\n",
    "\n",
    "        # in TFN we are doing a regression with constrained output range: (-3, 3), hence we'll apply sigmoid to output\n",
    "        # shrink it to (0, 1), and scale\\shift it back to range (-3, 3)\n",
    "        self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "        self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "\n",
    "    def forward(self, img_x, text_x):\n",
    "        '''\n",
    "        Args:\n",
    "            audio_x: tensor of shape (batch_size, audio_in)\n",
    "            video_x: tensor of shape (batch_size, video_in)\n",
    "            text_x: tensor of shape (batch_size, sequence_len, text_in)\n",
    "        '''\n",
    "        img_h = self.img_subnet(img_x)\n",
    "#         video_h = self.video_subnet(video_x)\n",
    "        text_h = self.text_subnet(text_x)\n",
    "        batch_size = img_h.data.shape[0]\n",
    "\n",
    "        # next we perform \"tensor fusion\", which is essentially appending 1s to the tensors and take Kronecker product\n",
    "        if img_h.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "\n",
    "        _img_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), img_h), dim=1)\n",
    "#         _video_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), video_h), dim=1)\n",
    "        _text_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), text_h), dim=1)\n",
    "\n",
    "        # _audio_h has shape (batch_size, audio_in + 1), _video_h has shape (batch_size, _video_in + 1)\n",
    "        # we want to perform outer product between the two batch, hence we unsqueenze them to get\n",
    "        # (batch_size, audio_in + 1, 1) X (batch_size, 1, video_in + 1)\n",
    "        # fusion_tensor will have shape (batch_size, audio_in + 1, video_in + 1)\n",
    "        fusion_tensor = torch.bmm(_img_h.unsqueeze(2), _text_h.unsqueeze(1))\n",
    "        \n",
    "        # next we do kronecker product between fusion_tensor and _text_h. This is even trickier\n",
    "        # we have to reshape the fusion tensor during the computation\n",
    "        # in the end we don't keep the 3-D tensor, instead we flatten it\n",
    "#         fusion_tensor = fusion_tensor.view(-1, (self.audio_hidden + 1) * (self.video_hidden + 1), 1)\n",
    "#         fusion_tensor = torch.bmm(fusion_tensor, _text_h.unsqueeze(1)).view(batch_size, -1)\n",
    "\n",
    "        post_fusion_dropped = self.post_fusion_dropout(fusion_tensor)\n",
    "        post_fusion_y_1 = F.relu(self.post_fusion_layer_1(post_fusion_dropped))\n",
    "        post_fusion_y_2 = F.relu(self.post_fusion_layer_2(post_fusion_y_1))\n",
    "        post_fusion_y_3 = F.sigmoid(self.post_fusion_layer_3(post_fusion_y_2))\n",
    "        output = post_fusion_y_3 * self.output_range + self.output_shift\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from model import TFN\n",
    "from utils import MultimodalDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess(dataset, epochs, model_path, max_len):\n",
    "    # parse the input args\n",
    "#     dataset = options['dataset']\n",
    "#     epochs = options['epochs']\n",
    "#     model_path = options['model_path']\n",
    "#     max_len = options['max_len']\n",
    "\n",
    "    # prepare the paths for storing models\n",
    "    model_path = os.path.join(\n",
    "        model_path, \"tfn.pt\")\n",
    "    print(\"Temp location for saving model: {}\".format(model_path))\n",
    "\n",
    "    # prepare the datasets\n",
    "    print(\"Currently using {} dataset.\".format(dataset))\n",
    "#     mosi = MultimodalDataset(dataset, max_len=max_len)\n",
    "\n",
    "# We Need to add our own data\n",
    "    train_set, valid_set, test_set = mosi.train_set, mosi.valid_set, mosi.test_set\n",
    "\n",
    "    audio_dim = train_set[0][0].shape[1]\n",
    "    print(\"Audio feature dimension is: {}\".format(audio_dim))\n",
    "    visual_dim = train_set[0][1].shape[1]\n",
    "    print(\"Visual feature dimension is: {}\".format(visual_dim))\n",
    "    text_dim = train_set[0][2].shape[1]\n",
    "    print(\"Text feature dimension is: {}\".format(text_dim))\n",
    "    input_dims = (audio_dim, visual_dim, text_dim)\n",
    "\n",
    "    # normalize the visual features\n",
    "    visual_max = np.max(np.max(np.abs(train_set.visual), axis=0), axis=0)\n",
    "    visual_max[visual_max==0] = 1\n",
    "    train_set.visual = train_set.visual / visual_max\n",
    "    valid_set.visual = valid_set.visual / visual_max\n",
    "    test_set.visual = test_set.visual / visual_max\n",
    "\n",
    "    # for visual and audio modality, we average across time\n",
    "    # here the original data has shape (max_len, num_examples, feature_dim)\n",
    "    # after averaging they become (1, num_examples, feature_dim)\n",
    "    train_set.visual = np.mean(train_set.visual, axis=0, keepdims=True)\n",
    "    train_set.audio = np.mean(train_set.audio, axis=0, keepdims=True)\n",
    "    valid_set.visual = np.mean(valid_set.visual, axis=0, keepdims=True)\n",
    "    valid_set.audio = np.mean(valid_set.audio, axis=0, keepdims=True)\n",
    "    test_set.visual = np.mean(test_set.visual, axis=0, keepdims=True)\n",
    "    test_set.audio = np.mean(test_set.audio, axis=0, keepdims=True)\n",
    "\n",
    "    # remove possible NaN values\n",
    "    train_set.visual[train_set.visual != train_set.visual] = 0\n",
    "    valid_set.visual[valid_set.visual != valid_set.visual] = 0\n",
    "    test_set.visual[test_set.visual != test_set.visual] = 0\n",
    "\n",
    "    train_set.audio[train_set.audio != train_set.audio] = 0\n",
    "    valid_set.audio[valid_set.audio != valid_set.audio] = 0\n",
    "    test_set.audio[test_set.audio != test_set.audio] = 0\n",
    "\n",
    "    return train_set, valid_set, test_set, input_dims\n",
    "\n",
    "def display(test_loss, test_binacc, test_precision, test_recall, test_f1, test_septacc, test_corr):\n",
    "    print(\"MAE on test set is {}\".format(test_loss))\n",
    "    print(\"Binary accuracy on test set is {}\".format(test_binacc))\n",
    "    print(\"Precision on test set is {}\".format(test_precision))\n",
    "    print(\"Recall on test set is {}\".format(test_recall))\n",
    "    print(\"F1 score on test set is {}\".format(test_f1))\n",
    "    print(\"Seven-class accuracy on test set is {}\".format(test_septacc))\n",
    "    print(\"Correlation w.r.t human evaluation on test set is {}\".format(test_corr))\n",
    "\n",
    "def main(options):\n",
    "    DTYPE = torch.FloatTensor\n",
    "    train_set, valid_set, test_set, input_dims = preprocess(options)\n",
    "\n",
    "    model = TFN(input_dims, (4, 16, 128), 64, (0.3, 0.3, 0.3, 0.3), 32)\n",
    "    if options['cuda']:\n",
    "        model = model.cuda()\n",
    "        DTYPE = torch.cuda.FloatTensor\n",
    "    print(\"Model initialized\")\n",
    "    criterion = nn.L1Loss(size_average=False)\n",
    "    optimizer = optim.Adam(list(model.parameters())[2:]) # don't optimize the first 2 params, they should be fixed (output_range and shift)\n",
    "    \n",
    "    # setup training\n",
    "    complete = True\n",
    "    min_valid_loss = float('Inf')\n",
    "    batch_sz = options['batch_size']\n",
    "    patience = options['patience']\n",
    "    epochs = options['epochs']\n",
    "    model_path = options['model_path']\n",
    "    train_iterator = DataLoader(train_set, batch_size=batch_sz, num_workers=4, shuffle=True)\n",
    "    valid_iterator = DataLoader(valid_set, batch_size=len(valid_set), num_workers=4, shuffle=True)\n",
    "    test_iterator = DataLoader(test_set, batch_size=len(test_set), num_workers=4, shuffle=True)\n",
    "    curr_patience = patience\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_iterator:\n",
    "            model.zero_grad()\n",
    "\n",
    "            # the provided data has format [batch_size, seq_len, feature_dim] or [batch_size, 1, feature_dim]\n",
    "            x = batch[:-1]\n",
    "            x_a = Variable(x[0].float().type(DTYPE), requires_grad=False).squeeze()\n",
    "            x_v = Variable(x[1].float().type(DTYPE), requires_grad=False).squeeze()\n",
    "            x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(batch[-1].view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "            output = model(x_a, x_v, x_t)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            train_loss += loss.data[0] / len(train_set)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch {} complete! Average Training loss: {}\".format(e, train_loss))\n",
    "\n",
    "        # Terminate the training process if run into NaN\n",
    "        if np.isnan(train_loss):\n",
    "            print(\"Training got into NaN values...\\n\\n\")\n",
    "            complete = False\n",
    "            break\n",
    "\n",
    "        # On validation set we don't have to compute metrics other than MAE and accuracy\n",
    "        model.eval()\n",
    "        for batch in valid_iterator:\n",
    "            x = batch[:-1]\n",
    "            x_a = Variable(x[0].float().type(DTYPE), requires_grad=False).squeeze()\n",
    "            x_v = Variable(x[1].float().type(DTYPE), requires_grad=False).squeeze()\n",
    "            x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(batch[-1].view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "            output = model(x_a, x_v, x_t)\n",
    "            valid_loss = criterion(output, y)\n",
    "        output_valid = output.cpu().data.numpy().reshape(-1)\n",
    "        y = y.cpu().data.numpy().reshape(-1)\n",
    "\n",
    "        if np.isnan(valid_loss.data[0]):\n",
    "            print(\"Training got into NaN values...\\n\\n\")\n",
    "            complete = False\n",
    "            break\n",
    "\n",
    "        valid_binacc = accuracy_score(output_valid>=0, y>=0)\n",
    "\n",
    "        print(\"Validation loss is: {}\".format(valid_loss.data[0] / len(valid_set)))\n",
    "        print(\"Validation binary accuracy is: {}\".format(valid_binacc))\n",
    "\n",
    "        if (valid_loss.data[0] < min_valid_loss):\n",
    "            curr_patience = patience\n",
    "            min_valid_loss = valid_loss.data[0]\n",
    "            torch.save(model, model_path)\n",
    "            print(\"Found new best model, saving to disk...\")\n",
    "        else:\n",
    "            curr_patience -= 1\n",
    "        \n",
    "        if curr_patience <= 0:\n",
    "            break\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    if complete:\n",
    "        \n",
    "        best_model = torch.load(model_path)\n",
    "        best_model.eval()\n",
    "        for batch in test_iterator:\n",
    "            x = batch[:-1]\n",
    "            x_a = Variable(x[0].float().type(DTYPE), requires_grad=False).squeeze()\n",
    "            x_v = Variable(x[1].float().type(DTYPE), requires_grad=False).squeeze()\n",
    "            x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(batch[-1].view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "            output_test = best_model(x_a, x_v, x_t)\n",
    "            loss_test = criterion(output_test, y)\n",
    "            test_loss = loss_test.data[0]\n",
    "        output_test = output_test.cpu().data.numpy().reshape(-1)\n",
    "        y = y.cpu().data.numpy().reshape(-1)\n",
    "\n",
    "        test_binacc = accuracy_score(output_test>=0, y>=0)\n",
    "        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(y>=0, output_test>=0, average='binary')\n",
    "        test_septacc = (output_test.round() == y.round()).mean()\n",
    "\n",
    "        # compute the correlation between true and predicted scores\n",
    "        test_corr = np.corrcoef([output_test, y])[0][1]  # corrcoef returns a matrix\n",
    "        test_loss = test_loss / len(test_set)\n",
    "\n",
    "        display(test_loss, test_binacc, test_precision, test_recall, test_f1, test_septacc, test_corr)\n",
    "    return\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     OPTIONS = argparse.ArgumentParser()\n",
    "#     OPTIONS.add_argument('--dataset', dest='dataset',\n",
    "#                          type=str, default='MOSI')\n",
    "#     OPTIONS.add_argument('--epochs', dest='epochs', type=int, default=50)\n",
    "#     OPTIONS.add_argument('--batch_size', dest='batch_size', type=int, default=32)\n",
    "#     OPTIONS.add_argument('--patience', dest='patience', type=int, default=20)\n",
    "#     OPTIONS.add_argument('--cuda', dest='cuda', type=bool, default=False)\n",
    "#     OPTIONS.add_argument('--model_path', dest='model_path',\n",
    "#                          type=str, default='models')\n",
    "#     OPTIONS.add_argument('--max_len', dest='max_len', type=int, default=20)\n",
    "#     PARAMS = vars(OPTIONS.parse_args())\n",
    "#     main(PARAMS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
